[
    {
        "cause_name": "workload_contention",
        "desc": "This code is designed to diagnose workload contention issues in a database system. The function checks for several potential causes of contention, including abnormal CPU and memory resource usage, insufficient space in the database data directory, and excessive connections or thread pool usage. If any of these issues are detected, the function provides a detailed report of the problem and suggests potential solutions. If no issues are found, the function returns \"not a root cause\".",
        "metrics": "- process_used_memory\n- max_process_memory\n- dynamic_used_memory\n- max_dynamic_memory\n- other_used_memory\n- tps\n- max_connections\n- db_cpu_usage\n- db_mem_usage\n- disk_usage\n- connection\n- thread_pool_rate"
    },
    {
        "cause_name": "database_wait_event",
        "desc": "This function checks if there is a wait event in the database. If there is a wait event, it retrieves the wait status and wait event information and stores it in the detail dictionary. If the detail dictionary already has wait event information, it suggests that there is no root cause for the issue. Otherwise, it suggests that the wait event may be a root cause for the issue. If there is no wait event information, it suggests that there is no root cause for the issue. Therefore, the presence of wait event information is a root cause for the issue, while the absence of wait event information is not a root cause.",
        "metrics": "- wait_event_info\n- wait_status\n- wait_event\n- detail\n- suggestion"
    },
    {
        "cause_name": "CPU_workload_analysis_and_diagnostics",
        "desc": "The code provided is analyzing the CPU workload and providing diagnostics based on certain conditions. \n\nFirst, the code retrieves CPU information using the \"getcpu\" function. It initializes variables \"sql_cpu\", \"flag_cpu\", and \"cpuinfo\". \n\nThen, it enters a loop to process each CPU resource. It concatenates SQL queries using the values from each resource, which will be used for further analysis. \n\nThe code then checks the value of the \"r\" attribute in each resource. If it is less than or equal to the variable \"cpun\" (representing the number of CPU threads), it indicates that the CPU load is not high. However, if it is greater than \"cpun\" but less than or equal to \"cpun\" multiplied by 2, it suggests that the CPU load is relatively high. If the value of \"r\" exceeds \"cpun\" multiplied by 2, it indicates a high CPU load with potential risks. The variable \"flag_cpu\" is incremented accordingly to keep track of the CPU load status.\n\nThe code then examines the value of the \"us\" attribute (representing CPU usage). If it is less than 20, it suggests that the CPU usage is not high. If it is between 20 and 80 (inclusive), it indicates normal CPU usage. If it is between 80 and 95 (inclusive), it suggests relatively high CPU usage. If the value exceeds 95 and the \"r\" value is greater than \"cpun\" multiplied by 2, it indicates a high CPU usage with a potential bottleneck. Otherwise, it concludes that the CPU is currently not experiencing performance bottlenecks.\n\nNext, the code evaluates the value of the \"sys\" attribute (representing system CPU usage). If it is less than 15, it suggests normal system CPU usage. If it is between 15 and 30 (inclusive), it indicates relatively high usage and requires attention to potential system issues. If it exceeds 30, it suggests significantly high system CPU usage, indicating the presence of serious vulnerabilities.\n\nBy analyzing the CPU information and applying various conditions, the code provides diagnostic information in the variable \"cpuinfo\". The variable \"flag_cpu\" keeps track of the number of identified CPU load issues.",
        "metrics": "- CPU workload\n- CPU information\n- CPU threads\n- CPU load\n- CPU usage\n- System CPU usage"
    },
    {
        "cause_name": "Memory_usage_analysis_diagnosis",
        "desc": "The code lines are analyzing and diagnosing the memory usage of a system. The code first retrieves the memory information using the \"getmem\" function. Then, it fetches the long-term memory information using the \"getltmem\" function with specific parameters. \n\nNext, the code initializes variables to store information about memory and swap usage. There are also flag variables that indicate whether there are any memory or swap issues.\n\nThe code then constructs a SQL query to retrieve specific memory information for each record obtained from the \"getmem\" function. Based on the retrieved information, the code analyzes the memory usage and assigns appropriate diagnostic messages to the \"mem_info\" variable. \n\nIf the physical memory usage exceeds 90% and the free memory is below 200M and above 100M, it indicates that the physical memory usage is high and the available memory is low, which may pose performance risks. Similarly, if the physical memory usage exceeds 90% and the free memory is below 100M, it suggests a high memory usage and a significant performance risk.\n\nIf the physical memory usage exceeds 20% of the recent average, it is recommended to perform further checks. \n\nThe code also checks the swap usage. If the swap usage is above 50% and below 80%, it indicates a high swap usage and suggests further investigation. If the swap usage exceeds 80%, it suggests a high swap usage and recommends immediate investigation.\n\nAfter analyzing each record, the code appends the diagnostic messages for memory and swap usage to the respective variables.\n\nFinally, the code removes the last \"union all\" from the constructed SQL query.\n\nIn conclusion, the code analyzes the memory and swap usage of a system and generates diagnostic messages based on the specific threshold values and recent average usage. These messages provide insights into any potential memory and swap issues that may affect the system's performance.",
        "metrics": "1. Physical memory usage\n2. Free memory\n3. Recent average physical memory usage\n4. Swap usage"
    },
    {
        "cause_name": "file_system_diagnostic_analysis",
        "desc": "The code provided is diagnosing the usage and availability of file systems. \n\nThe variable `fs` stores the file systems retrieved using the `getfs()` function. \n`fs_info` is an empty string that will be used to store diagnostic information about the file systems. \n`flag_fs` is an integer variable initialized to 0 and will be used to count the number of file systems with issues. \n\nThe code then enters a loop where it iterates over each file system in `fs`. Within the loop, the code appends SQL queries to the `sql_fs` string. These queries collect information about the file system, such as its name (`fsmp`) and percentage of disk space used (`fspct`).\n\nThe code then proceeds to analyze each file system individually. If the file system's disk usage is between 80% and 95% and the amount of free space (`fsres`) is less than 500MB, a diagnostic message is added to `fs_info` indicating that the file system's usage is high and its free space is low. The `flag_fs` variable is incremented to indicate that an issue has been detected.\n\nSimilarly, if the file system's usage is between 80% and 95% and the free space is greater than 5GB, another diagnostic message is added to `fs_info` indicating that the file system's usage is high but its free space is sufficient. `flag_fs` is incremented again in this case.\n\nIf the file system's usage is equal to or greater than 95%, a diagnostic message is added to `fs_info` indicating that the file system's usage exceeds the critical threshold.\n\nFinally, if none of the above conditions are true, which means the file system's usage is below 80%, a diagnostic message is added to `fs_info` stating that the file system's usage is normal.\n\nIn summary, this code diagnoses the usage and availability of file systems and provides relevant diagnostic information based on pre-defined thresholds. The `sql_fs` and `fs_info` variables store information for further analysis and reporting. The `flag_fs` variable keeps track of the number of file systems with issues identified.",
        "metrics": "1. File systems (fs)\n2. Disk space used (fspct)\n3. Free space (fsres)"
    },
    {
        "cause_name": "disk_group_usage_diagnostic_analysis",
        "desc": "The provided code appears to be a diagnostic script related to monitoring the usage of disk groups. The code iterates over a list called 'dg' and dynamically constructs a SQL query called 'sql_dg' by concatenating strings. The constructed query combines the names and percentages of disk groups ('dgname' and 'dgpct') using the \"union all\" clause.\n\nAfter constructing the SQL query, the code includes a series of conditional statements to analyze the disk group usage. If the percentage ('dgpct') is greater than 80 and less than 95, and the free space ('dgfree') is less than 2048, a diagnostic message is appended to the 'dg_info' string indicating that the disk group usage is high and the available space is low. The 'flag_dg' variable is incremented to indicate that a disk group exceeding the threshold has been identified.\n\nSimilarly, if the percentage is greater than 80 and less than 95, but the free space is greater than 10240, another diagnostic message is appended, indicating that the disk group usage is relatively high but the available space is sufficient. Again, 'flag_dg' is incremented.\n\nIf the percentage is equal to or greater than 95, a diagnostic message is appended to 'dg_info' indicating that the disk group usage is significantly exceeding the warning limit. 'flag_dg' is incremented accordingly.\n\nIf none of the above conditions are met, a message indicating normal disk group usage is appended to 'dg_info'.\n\nThe final line of the code trims off the trailing \"union all\" from the constructed SQL query.\n\nIn summary, this code analyzes the usage of disk groups and provides diagnostic messages based on defined thresholds and conditions. It iterates over a list of disk group information, constructs a SQL query, and appends diagnostic messages based on the calculated values.",
        "metrics": "- Percentage (dgpct): Represents the percentage usage of a disk group.\n- Free space (dgfree): Represents the amount of available free space in a disk group."
    },
    {
        "cause_name": "session_diagnostic_analysis",
        "desc": "The function retrieves session information from the database. It starts by initializing the variables for session percentage (sesspct), session information (sess_info), and a flag for session (flag_sess). \n\nNext, it iterates through each element in sesspct and constructs a SQL query based on the values retrieved. It also checks if the session percentage is above 90%. If it is, a diagnostic message is added to sess_info indicating that the session percentage is too high and requires attention. Additionally, the flag_sess variable is incremented. If the session percentage is below or equal to 90%, a different diagnostic message is added to sess_info indicating that the session percentage is normal.\n\nThe constructed SQL query is then truncated to remove the trailing \"union all\" statement.\n\nOverall, this code performs a diagnostic analysis on session information, checking for high session percentages and providing relevant diagnostic messages.",
        "metrics": "1. sesspct\n2. sess_info\n3. flag_sess\n4. session percentage\n5. SQL query\n6. diagnostic message\n7. trailing \"union all\" statement"
    },
    {
        "cause_name": "high_cpu_usage",
        "desc": "High CPU usage can cause slow SL execution and impact overall system performance",
        "steps": "1. Check the processes with high CPU usage using the 'top' command\n2. Use the 'perf' command to analyze the performance of the CPU\n3. Use the 'stackcollapse-perf.pl' tool to parse the perf data\n4. Fold the symbols in the 'perf.unfold' file using the 'stackcollapse-perf.pl' tool\n5. Generate a flame graph using the 'flamegraph.pl' tool\n6. Analyze the flame graph to identify functions with high width, indicating potential performance issues\n7. Contact technical support for further analysis",
        "metrics": "['CPU_utilization']"
    },
    {
        "cause_name": "high_cpu_usage_flame_graph",
        "desc": "Flame graph analysis can be used to identify functions with high CPU usage and potential performance issues",
        "steps": "1. Log in to the main DN node and check the processes with high CPU usage using the 'top' command\n2. Execute the 'perf' command to analyze the performance of the CPU\n3. Use the 'stackcollapse-perf.pl' tool to parse the perf data\n4. Fold the symbols in the 'perf.unfold' file using the 'stackcollapse-perf.pl' tool\n5. Generate a flame graph using the 'flamegraph.pl' tool\n6. Analyze the flame graph to identify functions with high width, indicating potential performance issues\n7. Contact technical support for further analysis",
        "metrics": "['CPU_utilization']"
    },
    {
        "cause_name": "slow_overall_performance",
        "desc": "The overall performance of the system is slow, causing high latency for business interfaces or not meeting customer expectations",
        "steps": "1. Understand the business background, such as customer expectations, business type, recent business changes, and whether the system has changed.",
        "metrics": "['P80', 'P95']"
    },
    {
        "cause_name": "high_cpu_usage",
        "desc": "High CPU usage can cause performance issues in the database system",
        "steps": "1. Login to the operation and maintenance management platform and check the CPU usage through instance monitoring or using the top command in the operating system on the database host. You can also use the sar command to view historical CPU usage.\n2. Identify the process with high CPU usage. If it is caused by the database, the expectation is that the database process will have a high usage.\n3. If the CPU is consistently high, analyze the SQL statements in the SQL ordered by CPU Time section of the WDR report for optimization.\n4. If the CPU usage is high for a short period of time, refer to the relevant sections in the overall performance slow query analysis.\n5. If the cause of CPU consumption cannot be determined, generate a flame graph for the abnormal time period and analyze the kernel code functions.",
        "metrics": "['cpu_usage']"
    },
    {
        "cause_name": "high_IO",
        "desc": "If the IO utilization is high or there are abnormal IO metrics such as high r await or high Vawalt, it may indicate IO issues. Possible causes include cache/RAID configuration problems or disk throttling. To analyze the IO consumption of threads, you can use tools like pidstat or iotop. If the IO consumption is mainly caused by TPLworker threads, it means that user SQL queries are consuming a lot of IO. You can use the lwtid from pg_thread_wait_status to find the corresponding tid and sessionid, and then use the tid/sessionid to find the session information and specific queries from pg_stat_activity. To optimize the queries and reduce IO consumption, refer to the performance tuning chapter.",
        "steps": "Check if the IO utilization is high or if there are abnormal IO metrics such as high r await or high Vawalt. If so, analyze the IO consumption of threads using tools like pidstat or iotop. If the IO consumption is mainly caused by TPLworker threads, use the lwtid from pg_thread_wait_status to find the corresponding tid and sessionid, and then use the tid/sessionid to find the session information and specific queries from pg_stat_activity. Optimize the queries to reduce IO consumption.",
        "metrics": "['IO_utilization', 'r_await', 'Vawalt']"
    },
    {
        "cause_name": "high_IO",
        "desc": "If the IO utilization is high or there are abnormal IO metrics such as high r await or high Vawalt, it may indicate IO issues. Possible causes include cache/RAID configuration problems or disk throttling. To analyze the IO consumption of threads, you can use tools like pidstat or iotop. If the IO consumption is mainly caused by TPLworker threads, it means that user SQL queries are consuming a lot of IO. You can use the lwtid from pg_thread_wait_status to find the corresponding tid and sessionid, and then use the tid/sessionid to find the session information and specific queries from pg_stat_activity. To optimize the queries and reduce IO consumption, refer to the performance tuning chapter.",
        "steps": "Check if the IO utilization is high or if there are abnormal IO metrics such as high r await or high Vawalt. If so, analyze the IO consumption of threads using tools like pidstat or iotop. If the IO consumption is mainly caused by TPLworker threads, use the lwtid from pg_thread_wait_status to find the corresponding tid and sessionid, and then use the tid/sessionid to find the session information and specific queries from pg_stat_activity. Optimize the queries to reduce IO consumption.",
        "metrics": "['IO_utilization', 'r_await', 'Vawalt']"
    },
    {
        "cause_name": "memory_full",
        "desc": "If the memory is full, it can cause slow program execution. To diagnose this issue, we need to identify the process with abnormal memory usage. In this case, we only consider the abnormal memory usage of the database process. For other processes, they are not representative and will not be described in detail here. To analyze the high memory usage of the database process, please refer to the relevant chapter on overall performance analysis.",
        "steps": "Check the memory usage of the database process. If it is significantly higher than normal, it may be the root cause of the slow program execution. Further analysis is needed to identify the specific reason for the abnormal memory usage.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "abnormal_persistent_events",
        "desc": "Abnormal persistent events in the database can impact overall performance. These events can be categorized as STATS, UNUCX EVENT, LOXXEVENT, and IO BVENT.",
        "steps": "Identifying abnormal persistent events can be an effective diagnostic method for overall performance issues. Refer to the chapter on persistent events in the overall performance slow analysis for detailed information on how to identify and address these events.",
        "metrics": "['persistent_events']"
    },
    {
        "cause_name": "long_term_performance_degradation",
        "desc": "Long-term performance degradation refers to a scenario where performance fluctuates over a certain period of time (e.g., hours). This can be identified by comparing performance reports from different time periods and analyzing differences in metrics such as top SQL, top wait events, load profile, cache/IO stats, and object stats.",
        "steps": "Compare performance reports from different time periods and analyze the differences in metrics mentioned above. Look for any significant changes or abnormalities that could explain the performance degradation. Refer to the chapter on analyzing performance reports for more details.",
        "metrics": "['top_sql', 'wait_events', 'load_profile', 'cache_stats', 'io_stats', 'object_stats']"
    },
    {
        "cause_name": "short_term_performance_fluctuation",
        "desc": "Short-term performance fluctuation refers to performance fluctuations that occur within a short period of time (e.g., seconds). These fluctuations may not be captured by default performance views, as they often show cumulative values. Refer to the chapter on analyzing performance fluctuations in the overall performance slow analysis for more information on how to diagnose and address short-term performance fluctuations.",
        "steps": "Refer to the chapter on analyzing performance fluctuations in the overall performance slow analysis for detailed steps on diagnosing and addressing short-term performance fluctuations.",
        "metrics": "['performance_fluctuation']"
    },
    {
        "cause_name": "high_cpu",
        "desc": "If the CPU usage is high, it can cause overall performance slowdown",
        "steps": "Check the CPU usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['cpu_usage']"
    },
    {
        "cause_name": "high_io",
        "desc": "If the IO usage is high, it can cause overall performance slowdown",
        "steps": "Check the IO usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['io_usage']"
    },
    {
        "cause_name": "high_memory",
        "desc": "If the memory usage is high, it can cause overall performance slowdown",
        "steps": "Check the memory usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "abnormal_events",
        "desc": "If there are abnormal events, such as concurrent updates, it can cause overall performance slowdown",
        "steps": "Check for any abnormal events, such as concurrent updates or other special events. If they are present, it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['abnormal_events']"
    },
    {
        "cause_name": "performance_jitter",
        "desc": "If there is performance jitter, it can cause overall performance slowdown",
        "steps": "Check for any performance jitter. If it is present, it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['performance_jitter']"
    },
    {
        "cause_name": "high_cpu",
        "desc": "High CPU usage in the database can be caused by poorly optimized SQL queries.",
        "steps": "1. Check the historical statement information for distributed or centralized databases using the 'dho pert.statemant' or 'dhbe pert.sumary statenent' query. Sort the results by CPU time to identify the queries with high CPU usage.\n2. If the CPU usage is currently high, use the 'pt stat activlty' query to get the query ID of the running SQL. Then use the query ID to query the 'pa threadwait status' to get the lvtid of the running SQL.\n3. Use the 'top -Hp gaussdh\u8fdb\u7a0b\u53f7' command to check the CPU usage of the corresponding lvtid.\n4. If the CPU usage is confirmed to be high, the identified SQL query is the target. If the SQL query has been running for a long time, check the performance fluctuations section to identify the target SQL.\n5. Query the statement history table or use the 'dbe perf.get global full sql by tisestamp' query to find slow SQL queries with high CPU usage.\n6. If the high CPU usage is intermittent, use the dynamic interface to capture detailed information of the subsequent execution of the query. Use the 'dynanic fune control' query to capture and cancel the capture of specific SQL queries.\n7. After using the dynamic interface, check the statement history table for the captured SQL queries.",
        "metrics": "['cpu_usage', 'query_id', 'lvtid']"
    },
    {
        "cause_name": "high_io",
        "desc": "High IO can be caused by user statements that result in excessive physical reads. It can be identified by checking the 'n blocks fetched' and 'n blocks hit' fields in the dbe_pert_statement/dhe_pert_summary_statement views. If the difference between these two fields is high, it indicates a high number of physical reads. Additionally, the 'wait status' and 'wait event' fields in the pg_thread_wait_status view can be checked to see if there are any IO-related events or data file reads. The dhe_perf_local_active_session/gs_asp views can also be queried to check for records with Query events related to 'IO EVENT/DataFileRead' during a specific time period. Slow SQL queries with a high difference in 'n blocks fetched' and 'n blocks hit' fields or high data IO time can also indicate high IO. In the case of slow SQL, the 'details' field may contain information about the events causing the high IO.",
        "steps": "1. Check the 'n blocks fetched' and 'n blocks hit' fields in the dbe_pert_statement/dhe_pert_summary_statement views. If the difference between these two fields is high, it indicates a high number of physical reads. 2. Check the 'wait status' and 'wait event' fields in the pg_thread_wait_status view to see if there are any IO-related events or data file reads. 3. Query the dhe_perf_local_active_session/gs_asp views to check for records with Query events related to 'IO EVENT/DataFileRead' during a specific time period. 4. Check for slow SQL queries with a high difference in 'n blocks fetched' and 'n blocks hit' fields or high data IO time. If available, check the 'details' field for information about the events causing the high IO.",
        "metrics": "['n_blocks_fetched', 'n_blocks_hit', 'wait_status', 'wait_event', 'data_io_time']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "High memory usage in the database can lead to performance issues. It is important to analyze and identify the specific areas of memory consumption.",
        "steps": "1. Query the perf.memory node.detail view to determine the memory usage points, such as the maximum process memory, process used memory, maximum dynamic memory, dynamic used memory, and dynamic used shared memory. Pay attention to the difference between max dynamic memory and dynamic used memory. If the dynamic memory is insufficient, it can cause query errors. The dynamic used memory includes memory consumption on user sessions and memory consumption by kernel modules.\n2. If dynamic used shared memory is small, query the perf.session memory detail view to get the memory consumption of different sessions. The number of user sessions and the memory usage per session can cause dynamic memory issues.\n3. If dynamic used shared memory is large, query the perf.shared memory detail view to identify the context of abnormal memory consumption. In most cases, it is caused by abnormal memory consumption on user sessions.",
        "metrics": "['max_process_memory', 'process_used_memory', 'max_dynamic_memory', 'dynamic_used_memory', 'dynamic_used_shared_memory']"
    },
    {
        "cause_name": "slow_queries",
        "desc": "Slow queries can cause overall performance degradation. It is important to identify the specific wait events and analyze their impact on performance.",
        "steps": "1. For current slow performance, query the pg_thread_wait_status view to get the wait events that most sessions are waiting for.\n2. For past slow performance within a short period of time, query the perf.local_active_session view to analyze the events that occurred during that time.\n3. For persistent slow performance, query the perf.mlt_sonts view and sort by total wait time to identify the top events.",
        "metrics": "['wait_status', 'event']"
    },
    {
        "cause_name": "performance_jitter",
        "desc": "Performance jitter refers to fluctuations in performance over a period of time. It can be analyzed at different time scales.",
        "steps": "1. For performance jitter at the hour level, use the W0R analysis.\n2. For performance jitter at the minute level, analyze the Active Session Profile (ASP) views and tables.\nNote: ASP samples active session information and stores it in memory (perf.local_active_session). The ideal scenario is to store session data every second in memory and store it in physical tables at IO second intervals.",
        "metrics": "['W0R', 'ASP']"
    },
    {
        "cause_name": "performance_slow",
        "desc": "The overall performance is slow, which does not meet the latency requirements of customer jobs or customer expectations.",
        "steps": "1. Connect to the CV node of the database and query relevant views. For centralized databases, connect to the main DN node; for distributed databases, connect to the CX node. 2. Analyze the wait events in the following performance views: dhe_pert.mit_events, pg_thread_wait_status, statement_history, dbe_perf.local_active_session/gs_asp. 3. Interpret the meanings of different wait events and identify potential bottlenecks. 4. Take appropriate actions based on the analysis results.",
        "metrics": "['P80', 'P95']"
    },
    {
        "cause_name": "performance_slow_wait_events",
        "desc": "The overall performance is slow, and the wait events need to be analyzed to identify potential bottlenecks.",
        "steps": "1. Connect to the CV node of the database and query relevant views. For centralized databases, connect to the main DN node; for distributed databases, connect to the CX node. 2. Analyze the wait events in the following performance views: dhe_pert.mit_events, pg_thread_wait_status, statement_history, dbe_perf.local_active_session/gs_asp. 3. Interpret the meanings of different wait events and identify potential bottlenecks. 4. Take appropriate actions based on the analysis results.",
        "metrics": "['P80', 'P95']"
    },
    {
        "cause_name": "concurrent_query_optimization",
        "desc": "GaussDB supports optimizing complex query execution using operator concurrency when system CU, memory, and IO resources are sufficient.",
        "steps": "To configure concurrent query optimization, follow these steps: 1. Observe the current system load. If the system resources are sufficient (resource utilization is less than 60%), proceed to step 2. Otherwise, exit. 2. Set query_dop to 1 (default value) and use wplain to print the execution plan. Observe whether the plan meets the applicable scenarios in the 'Performance Tuning > System Tuning Guide > SIP Applicable Scenarios and Limitations' section of the Administrator's Guide. If it does, proceed to step 3. 3. Set the query_dop value without considering resource conditions and plan characteristics, and forcibly select dp as 1 or ralue. 4. Set the appropriate query_dop value before executing the query statement that meets the conditions, and check the query_dop after the statement execution is completed. For example, 'gaussdb-t SET query_dop * 4: gaussdb-s SELECT COUNT(v) FROM tI GRQP BY a;'. Note: - In the case of sufficient resources, the higher the parallelism, the better the performance improvement. - The SP parallelism supports session-level settings. It is recommended that customers open SP before executing queries that meet the requirements and close SP after execution to avoid impacting business during peak hours.",
        "metrics": "['query_dop']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "If the database process memory usage is consistently high, it can indicate memory overload.",
        "steps": "1. Observe the memory usage trend of the database process on the monitoring platform. If the memory usage remains high for a long time, regardless of whether there is any business running, it may indicate memory overload.\n2. Check if the memory usage increases significantly during the execution of jobs. If the memory usage spikes during job execution and then returns to a lower level after the job finishes, it may indicate memory overload.\n3. Check if the memory usage continues to increase during the execution of SQL statements and does not decrease afterwards. This may indicate memory accumulation.\n4. If SQL statements report out-of-memory errors, it may indicate memory shortage.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "high_concurrency_memory_usage",
        "desc": "If the number of connections to the server is too high, it can lead to high memory usage.",
        "steps": "Check if the server has a high number of connections, which can be observed by monitoring the memory usage and checking if it increases significantly when there are many connections.",
        "metrics": "['memory_usage', 'connection_count']"
    },
    {
        "cause_name": "high_memory_usage_sql",
        "desc": "Some SQL statements may have high memory usage, leading to temporary memory spikes.",
        "steps": "Check if certain SQL statements have high memory usage. This can be observed by monitoring the memory usage and checking if it spikes during the execution of specific SQL statements.",
        "metrics": "['memory_usage', 'sql_memory_usage']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "If the database process memory usage is consistently high, it can indicate memory overload.",
        "steps": "1. Observe the memory usage trend of the database process on the monitoring platform. If the memory usage remains high for a long time, regardless of whether there is any business running, it may indicate memory overload.\n2. Check if the memory usage increases significantly during the execution of jobs. If the memory usage spikes during job execution and then returns to a lower level after the job finishes, it may indicate memory overload.\n3. Check if the memory usage continues to increase during the execution of SQL statements and does not decrease afterwards. This may indicate memory accumulation.\n4. If SQL statements report out-of-memory errors, it may indicate memory shortage.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "memory_overload",
        "desc": "If the memory usage of the database process is high, it can be caused by memory accumulation or other reasons",
        "steps": "1. Query the memory statistics information to get an overview of the memory usage.\n2. Determine the memory usage classification based on the memory statistics.\n3. Determine the cause of memory accumulation based on the memory usage classification.",
        "metrics": "['process_used_memory', 'dynamic_used_memory', 'dynamic_used_shrctx', 'shared_used_memory', 'other_used_memory']"
    },
    {
        "cause_name": "memory_overload",
        "desc": "In some cases, memory overload can cause cluster restarts, making it difficult to locate the cause of memory overload in real-time. In such cases, the following steps can be used to identify the cause of memory overload that has occurred in the past.",
        "steps": "1. Check the historical memory usage curve to identify the time point of memory overload. 2. Analyze the historical memory statistics to find the memory context with high usage. 3. Contact engineers for assistance in resolving the memory overload issue.",
        "metrics": "['memory_usage', 'memory_context']"
    },
    {
        "cause_name": "memory_usage_classification",
        "desc": "Based on the memory usage information obtained from the query, the memory usage can be classified as follows: If dynamic used memory is large and dynamic used shared memory is small, it indicates that there is a high memory usage on threads and sessions. If dynamic used memory is large and dynamic used shared memory is similar, it indicates that there is a large dynamic memory usage on the global memory context. If only shared used memory is large, it indicates that there is a high usage of shared memory, which can be ignored. If other used memory is large, it is usually due to frequent memory allocation and deallocation during business execution, leading to excessive memory fragmentation.",
        "steps": "For each memory usage scenario, analyze the values of the relevant metrics. If the conditions mentioned in the content are met, it can be confirmed that the corresponding memory context is causing high memory usage. Further steps are provided for each scenario to locate the specific code or snapshot causing the memory accumulation.",
        "metrics": "['dynamic_used_memory', 'dynamic_used_shared_memory', 'shared_used_memory', 'other_used_memory']"
    },
    {
        "cause_name": "slow_sql_execution",
        "desc": "The SQL execution is slow and does not meet customer expectations and business requirements",
        "steps": "1. Identify the currently running SQL and their corresponding session ID, PID, and node name. 2. Determine the main wait events causing the slow SQL execution. 3. If most of the wait events are on the wait node, check the specific wait events on that node. 4. If the top wait event is related to acquiring locks, find the blocking session and investigate further. 5. Analyze the top wait events and combine them with the execution plan to identify the performance bottleneck. 6. Use the statement history to analyze historical slow SQL and identify the main time-consuming stages. 7. If the execution time does not exceed the log_min_duration_statement threshold, consider enabling full SQL tracking or using dynamic interfaces to capture the slow SQL.",
        "metrics": "['response_time', 'execution_time']"
    },
    {
        "cause_name": "suboptimal_query_plan",
        "desc": "The query plan for the slow SQL is suboptimal, leading to poor performance",
        "steps": "1. Obtain the execution plan for the slow SQL. 2. Analyze the execution plan to identify any suboptimal steps or operations. 3. Optimize the query plan by adjusting query parameters or rewriting the SQL statement.",
        "metrics": "['execution_plan']"
    },
    {
        "cause_name": "overall_performance_slow",
        "desc": "The overall performance is slow and does not meet the customer's latency requirements or expectations. This can be caused by various reasons such as business-side issues, insufficient system resources, suboptimal usage of database kernel resources, concurrency issues, suboptimal database configuration, and non-optimal SQL.",
        "steps": "1. Understand the business background, such as customer expectations, business type, recent business changes, and whether the system has undergone any changes.",
        "metrics": "['P80', 'P95']"
    },
    {
        "cause_name": "high_cpu_usage",
        "desc": "High CPU usage can cause performance issues in the database system.",
        "steps": "1. Login to the operation and maintenance management platform and check the CPU usage through instance monitoring or use the top command in the operating system to check the CPU usage on the database host. You can also use the sar command to view historical CPU usage.\n2. Identify the process with high CPU usage. If it is caused by the database, the expectation is that the database process will have a higher usage.\n3. If the CPU is consistently high, analyze the SQL statements in the SQL ordered by CPU Time section of the WDR report for optimization.\n4. If the CPU usage is high for a short period of time, refer to the relevant sections in the overall performance slow query analysis.\n5. If the cause of CPU consumption cannot be determined, generate a flame graph for the abnormal time period and analyze the kernel code functions.",
        "metrics": "['cpu_usage']"
    },
    {
        "cause_name": "high_IO",
        "desc": "If the IO utilization is high or there are abnormal IO metrics such as high r await or high Vawalt, it may indicate IO issues. Possible causes include cache/raid configuration problems or disk throttling. To analyze the IO consumption of threads, you can use tools like pidstat or iotop. If the IO consumption is abnormal for TPLworker threads, it means that user SQL queries are consuming a lot of IO. You can use the lwtid from pg_thread_wait_status view to find the corresponding session information and optimize the queries to reduce IO consumption.",
        "steps": "1. Check if the IO utilization is high or if there are abnormal IO metrics such as high r await or high Vawalt. If yes, proceed to the next step. Otherwise, it is not a root cause.\n2. Analyze the IO consumption of threads using tools like pidstat or iotop.\n3. If the IO consumption is abnormal for TPLworker threads, use the lwtid from pg_thread_wait_status view to find the corresponding session information.\n4. Optimize the queries to reduce IO consumption.",
        "metrics": "['IO_utilization', 'r_await', 'Vawalt']"
    },
    {
        "cause_name": "abnormal_persistent_events",
        "desc": "Abnormal persistent events in the database can be categorized as STATS, UNUCX EVENT, LOXXEVENT, and IO BVENT. Identifying abnormal persistent events can be an effective way to diagnose overall performance slow query issues.",
        "steps": "Refer to the chapter on analyzing persistent events in the overall performance slow query analysis section for detailed information.",
        "metrics": "['STATS', 'UNUCX_EVENT', 'LOXXEVENT', 'IO_BVENT']"
    },
    {
        "cause_name": "long_term_performance_degradation",
        "desc": "Long-term performance degradation refers to a scenario where performance fluctuates significantly within a certain period of time (e.g., hours). To diagnose this issue, compare performance reports and investigate the following items: Top SQL, Top Wait Events, Load Profile, Cache/IO Stats, and Object Stats.",
        "steps": "Compare performance reports from different time periods and analyze the differences. Focus on the mentioned metrics to identify potential causes of performance degradation.",
        "metrics": "['Top_SQL', 'Top_Wait_Events', 'Load_Profile', 'Cache_IO_Stats', 'Object_Stats']"
    },
    {
        "cause_name": "short_term_performance_fluctuation",
        "desc": "Short-term performance fluctuation refers to a scenario where performance fluctuates at a finer granularity (e.g., seconds). This issue may not be captured by default performance views, but can be analyzed using the techniques described in the chapter on analyzing performance fluctuations in the overall performance slow query analysis section.",
        "steps": "Refer to the chapter on analyzing performance fluctuations in the overall performance slow query analysis section for detailed information on diagnosing short-term performance fluctuations.",
        "metrics": "[]"
    },
    {
        "cause_name": "suboptimal_sql",
        "desc": "Suboptimal SQL queries can negatively impact overall performance and potentially saturate the database's thread pool. Identifying suboptimal SQL queries is important for performance optimization.",
        "steps": "To identify suboptimal SQL queries, check the pg_stat_activity view for slow queries. Consider the execution time of the queries to determine their impact on performance. Additionally, analyze the OPS slow SQL metrics and use the statement_history table in the postgres database to find target slow queries.",
        "metrics": "[]"
    },
    {
        "cause_name": "high_cpu",
        "desc": "If the CPU usage is high, it can cause overall performance degradation",
        "steps": "Check the CPU usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['cpu_usage']"
    },
    {
        "cause_name": "high_io",
        "desc": "If the IO usage is high, it can cause overall performance degradation",
        "steps": "Check the IO usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['io_usage']"
    },
    {
        "cause_name": "high_memory",
        "desc": "If the memory usage is high, it can cause overall performance degradation",
        "steps": "Check the memory usage. If it exceeds the threshold (80%), it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "abnormal_events",
        "desc": "If there are abnormal events, such as concurrent updates, it can cause overall performance degradation",
        "steps": "Check for any abnormal events, such as concurrent updates or other special events. If they are present, it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['abnormal_events']"
    },
    {
        "cause_name": "performance_jitter",
        "desc": "If there is performance jitter, it can cause overall performance degradation",
        "steps": "Check for any performance jitter. If it is present, it is considered a root cause. Possible solutions include optimizing queries, adding indexes, or upgrading hardware.",
        "metrics": "['performance_jitter']"
    },
    {
        "cause_name": "high_cpu",
        "desc": "High CPU usage in the database can be caused by poorly optimized SQL queries.",
        "steps": "1. Check the historical statement information for distributed or centralized databases using the 'dho pert.statemant' or 'dhbe pert.sumary statenent' query. Sort the results by CPU time to identify the queries with high CPU usage.\n2. If the CPU usage is currently high, use the 'pt stat activlty' query to get the query ID of the running SQL. Then use the query ID to query the 'pa threadwait status' to get the lvtid of the running SQL.\n3. Use the 'top -Hp gaussdh\u8fdb\u7a0b\u53f7' command to check the CPU usage of the corresponding lvtid.\n4. If the CPU usage is confirmed to be high, the identified SQL query is the target. If the SQL query has been running for a long time, check the performance fluctuations section to identify the target SQL.\n5. Query the statement history table or use the 'dbe perf.get global full sql by tisestamp' query to find slow SQL queries with high CPU usage.\n6. If the high CPU usage is intermittent, use the dynamic interface to capture detailed information of the subsequent execution of the query. Use the 'dynanic fune control' query to capture and cancel the capture of specific SQL queries.\n7. After using the dynamic interface, check the statement history table for the captured SQL queries.",
        "metrics": "['cpu_usage', 'query_id', 'lvtid']"
    },
    {
        "cause_name": "high_cpu_user_statement",
        "desc": "High CPU usage in the database can be caused by poorly optimized user SQL statements.",
        "steps": "1. Check the historical statement information for distributed or centralized databases using the 'dho pert.statemant' or 'dhbe pert.sumary statenent' queries. Sort the results by CPU time to identify the queries with high CPU usage.\n2. If the CPU usage is currently high, use the 'pt stat activlty' query to get the query ID of the running SQL. Then use the query ID to query the 'pa threadwait status' to get the lvtid of the running SQL.\n3. Use the 'top -Hp gaussdh\u8fdb\u7a0b\u53f7' command to check the CPU usage of the corresponding lvtid.\n4. If the CPU usage is confirmed to be high, the identified SQL query is the target. If the SQL query has been running for a long time, the thread ID will remain the same.\n5. If there is a high CPU usage in the past, refer to the performance jitter section to identify the target SQL.\n6. Query the statement history table or use the 'dbe perf.get global full sql by tisestamp' query to find slow SQL queries with high CPU usage.\n7. If the identified SQL query has intermittent high CPU usage, use the dynamic interface to capture detailed information about subsequent executions of the query.\n8. Use the dynamic interface commands to cancel the capture of specific SQL queries or all SQL queries.\n9. After using the dynamic interface, check the statement history table for any recorded SQL queries.\n10. Evaluate the execution frequency of the target SQL query before enabling the dynamic interface. Clean up all SQL queries in the dynamic interface after use.",
        "metrics": "['cpu_usage', 'query_id', 'lvtid']"
    },
    {
        "cause_name": "high_io",
        "desc": "High IO can be caused by user statements that result in excessive physical reads. It can be identified by checking the 'n blocks fetched' and 'n blocks hit' fields in the dbe_pert_statement/dhe_pert_summary_statement views. If the difference between these two fields is high, it indicates a high number of physical reads. Additionally, the 'wait status' and 'wait event' fields in the pg_thread_wait_status view can be checked to see if there are any IO-related events or data file reads. The dhe_perf_local_active_session/gs_asp views can also be queried to check for records with Query events related to 'IO EVENT/DataFileRead' during a specific time period. Slow SQL queries with a high difference in 'n blocks fetched' and 'n blocks hit' fields or high data IO time can also indicate high IO. In the case of slow SQL, the 'details' field may contain information about the events causing the high IO.",
        "steps": "1. Check the 'n blocks fetched' and 'n blocks hit' fields in the dbe_pert_statement/dhe_pert_summary_statement views. If the difference between these two fields is high, it indicates a high number of physical reads. 2. Check the 'wait status' and 'wait event' fields in the pg_thread_wait_status view to see if there are any IO-related events or data file reads. 3. Query the dhe_perf_local_active_session/gs_asp views to check for records with Query events related to 'IO EVENT/DataFileRead' during a specific time period. 4. Check for slow SQL queries with a high difference in 'n blocks fetched' and 'n blocks hit' fields or high data IO time. If available, check the 'details' field for information about the events causing the high IO.",
        "metrics": "['n_blocks_fetched', 'n_blocks_hit', 'wait_status', 'wait_event', 'data_io_time']"
    },
    {
        "cause_name": "performance_jitter",
        "desc": "Performance jitter refers to fluctuations in performance over a period of time. It can be analyzed at different time scales.",
        "steps": "1. For performance jitter at the hour level, use the W0R analysis.\n2. For performance jitter at the minute level, analyze the Active Session Profile (ASP) views and tables.\nNote: ASP samples active session information and stores it in memory (perf.local_active_session). The ideal scenario is to store session data every second in memory and store it in physical tables at IO second intervals.",
        "metrics": "['W0R', 'ASP']"
    },
    {
        "cause_name": "concurrent_query_optimization",
        "desc": "GaussDB supports optimizing complex query execution using operator concurrency when system CU, memory, and IO resources are sufficient.",
        "steps": "To configure concurrent query optimization, follow these steps: 1. Observe the current system load. If the system resources are sufficient (resource utilization is less than 60%), proceed to step 2. Otherwise, exit. 2. Set query_dop to 1 (default value) and use wplain to print the execution plan. Observe whether the plan meets the applicable scenarios in the 'Performance Tuning > System Tuning Guide > SIP Applicable Scenarios and Limitations' section of the Administrator's Guide. If it does, proceed to step 3. 3. Set the query_dop value without considering resource conditions and plan characteristics, and forcibly select dp as 1 or ralue. 4. Set the appropriate query_dop value before executing the query statement that meets the conditions, and check the query_dop after the statement execution is completed. For example, 'gaussdb-t SET query_dop * 4: gaussdb-s SELECT COUNT(v) FROM tI GRQP BY a;'. Note: - In the case of sufficient resources, the higher the parallelism, the better the performance improvement. - The SP parallelism supports session-level settings. It is recommended that customers open SP before executing queries that meet the requirements and close SP after execution to avoid impacting business during peak hours.",
        "metrics": "['query_dop']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "If the database process memory usage is consistently high, it can indicate memory overload.",
        "steps": "1. Observe the memory usage trend of the database process on the monitoring platform. If the memory usage remains high for a long time, regardless of whether there is any business running, it may indicate memory overload.\n2. Check if the memory usage increases significantly during the execution of jobs. If the memory usage spikes during job execution and then returns to a lower level after the job finishes, it may indicate memory overload.\n3. Check if the memory usage continues to increase during the execution of SQL statements and does not decrease afterwards. This may indicate memory accumulation.\n4. If SQL statements report out-of-memory errors, it may indicate memory shortage.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "memory_overload",
        "desc": "In some cases, memory overload can cause cluster environment restart, making it difficult to locate the cause of memory overload in real-time. In such cases, the following steps can be used to locate the cause of memory overload that has occurred in the past.",
        "steps": "1. Check the historical memory usage curve to identify the time point of memory overload. 2. Analyze the historical memory statistics information to find the memory context with high usage. 3. Contact engineers for assistance in resolving the memory overload issue.",
        "metrics": "['memory_usage', 'memory_context']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "High memory usage can be caused by various reasons, including excessive memory fragmentation, delayed memory release, and other factors.",
        "steps": "1. Check if there is excessive memory fragmentation, which can be caused by frequent allocation and deallocation of memory, such as creating a large number of cache plans. If so, consider optimizing the memory allocation and deallocation process.\n2. Investigate if there are any third-party open-source software that is not releasing memory in a timely manner. If found, contact the engineer for assistance.\n3. If the high memory usage is not solely caused by memory fragmentation, it could be due to other reasons. For example, there may be business logic that directly uses memory without proper management, or there may be other factors causing delayed memory release. In such cases, contact the engineer for further investigation and resolution.",
        "metrics": "['other_used_memory']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "In the scenario of parallel decoding, if the reading log thread or decoding thread consumes too much memory, it can lead to insufficient memory and trigger memory error. The memory usage of ParallelDeeokeoispatcher or ParallelDecodelog is found to be relatively high.",
        "steps": "For the reading log thread, it is recommended to increase the number of decoding threads. For the decoding thread, it is suggested to set the 'r-tm-in aeory' parameter to adjust the buffer size and refer to the 'Parallel Decoding Options' section in the 'Feature Guide' for more details.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "In the scenario of parallel decoding, if the reading log thread or decoding thread consumes too much memory, it can lead to insufficient memory and trigger memory error. The memory usage of ParallelDeeokeoispatcher or ParallelDecodelog is found to be relatively high.",
        "steps": "For the reading log thread, it is recommended to increase the number of decoding threads. For the decoding thread, it is suggested to set the 'r-tm-in aeory' parameter to adjust the buffer size and refer to the 'Parallel Decoding Options' section in the 'Feature Guide' for more details.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "suboptimal_sql_plan",
        "desc": "If the SQL plan chosen for the query is not optimal, it can cause slow SQL performance on partitioned tables",
        "steps": "Examine the execution plan of the query to see if it is using the most efficient plan. Look for any sequential scans or suboptimal join strategies. Consider using query hints or modifying the query to improve the plan. It may also be helpful to analyze the statistics of the partitioned table to ensure accurate cardinality estimates.",
        "metrics": "['execution_plan']"
    },
    {
        "cause_name": "high_cpu_usage",
        "desc": "High CPU usage can be caused by the gaussdb process or certain SQL statements. It can be diagnosed by checking the CPU usage of the system and analyzing the WDR report.",
        "steps": "1. Check the CPU usage of the system using tools like OPSCPU, top command, or sar command to identify the high CPU usage process. If it is caused by the database, the gaussdb process is expected to have a high usage.\n2. Compare the WDR report of normal time periods and abnormal time periods to analyze the SQL statements in the 'Top SQL order by CPU' section.\n3. If the CPU usage is consistently high, try optimizing the SQL statements mentioned in the 'SQL ordered by CPU Time' section of the WDR report. Alternatively, refer to Chapter 1.3 for further analysis.\n4. If the cause of high CPU usage is still unclear, generate a flame graph for the database code functions during the abnormal time period and identify the bottleneck points. Refer to flame graph analysis for guidance.",
        "metrics": "['cpu_usage', 'gaussdb_process', 'top_sql_cpu']"
    },
    {
        "cause_name": "high_IO",
        "desc": "If the system shows high IO utilization, it can indicate potential performance issues. This can be observed through metrics such as %util in iostat, high r_await (usually greater than 3ms), or high w_await (usually greater than 3ms).",
        "steps": "If the above IO metrics show abnormalities, such as low read/write throughput or high latency, it is recommended to contact the operating system team for further analysis. Possible causes may include: (1) misconfiguration of disk cache/RAID write strategy, (2) disk bandwidth throttling (OBS itself has flow control). Additionally, if the IO volume is high, tools like pidstat or iotop can be used to analyze the threads consuming IO. Specifically, the TPLworker thread often indicates excessive IO caused by user SQL queries. To identify the SQL statements causing high IO, follow these steps: (1) Identify the TID (Thread ID) using pidstat/iotop, (2) Query the pg_thread_wait_status view with the lwtid obtained in the previous step to get the corresponding tid and sessionid, (3) Query the pg_stat_activity view with the tid and sessionid obtained in the previous step to find the session information causing high IO, including the specific SQL statement. Once identified, optimize the related queries to reduce IO volume, referring to Chapter 1.4 for guidance. Additionally, the SQL ordered by Physical Reads section in the WDR report can help identify queries causing high IO during specific time periods. If the IO volume remains consistently high and is caused by user statements, refer to the IO-related content in Chapter 1.3 for further assistance.",
        "metrics": "['%util', 'r_await', 'w_await']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "If the memory usage of the database system is high, it can cause slow program execution.",
        "steps": "First, identify the process with abnormal memory usage. In this case, we only consider the gaussdb process. Refer to the 'SQL ordered by CPU Time' section in the WDR report to analyze and optimize the relevant statements. Alternatively, refer to Chapter 1.3 for short-term CPU abnormalities.",
        "metrics": "['memory_usage']"
    },
    {
        "cause_name": "suboptimal_database_usage",
        "desc": "Suboptimal database usage can lead to performance degradation and inefficiency.",
        "steps": "To diagnose suboptimal database usage, refer to Chapter 1.3 for guidance.",
        "metrics": "['latency', 'tps', 'thread_pool']"
    },
    {
        "cause_name": "abnormal_wait_events",
        "desc": "Abnormal wait events in the database can indicate performance issues",
        "steps": "To identify abnormal wait events, check the wait events in the database. If there are wait events that are significantly higher than normal, it indicates a performance issue. Refer to Chapter 1.2 for more details on how to identify and diagnose abnormal wait events.",
        "metrics": "['wait_events']"
    },
    {
        "cause_name": "long_term_performance_degradation",
        "desc": "Long-term performance degradation refers to a scenario where the performance of the database fluctuates significantly over a certain period of time (hours). For example, the performance is normal from 8:00 to 9:00, but there is a significant performance fluctuation from 10:00 to 11:00. In this scenario, we can compare the WDR reports of the two time periods to identify any differences. The following aspects can be investigated: (1) Top SQL queries, (2) Top wait events, (3) Load profile, (4) Cache/IO statistics, (5) Object statistics.",
        "steps": "1. Compare the WDR reports of the two time periods to identify any differences.\n2. Investigate the top SQL queries during the time periods.\n3. Analyze the top wait events to identify any bottlenecks.\n4. Examine the load profile to understand the workload during the time periods.\n5. Check the cache/IO statistics to identify any issues with caching or IO performance.\n6. Analyze the object statistics to identify any issues with specific database objects.",
        "metrics": "['WDR_reports', 'Top_SQL', 'Top_wait_events', 'Load_profile', 'Cache_IO_stats', 'Object_stats']"
    },
    {
        "cause_name": "short_term_performance_jitter",
        "desc": "Short-term performance jitter refers to the situation where there are sudden fluctuations in performance at the second level. This is not adequately captured by the default one-hour interval of the WDR snapshot. It can be diagnosed by referring to the chapter on overall performance slow - analyzing performance jitter.",
        "steps": "Refer to the chapter on overall performance slow - analyzing performance jitter for diagnosis steps.",
        "metrics": "[]"
    },
    {
        "cause_name": "io_abnormality",
        "desc": "This event indicates an IO abnormality. Possible causes include IO issues or inefficient IO usage due to business operations. Investigate and analyze further.",
        "steps": "Check if there is a large number of IO events. Investigate possible causes such as IO issues or inefficient IO usage due to business operations.",
        "metrics": "['IO_EVENT/DataFileRead', 'IO_EVENT/DataFileWrite']"
    },
    {
        "cause_name": "high_cpu_user_sql",
        "desc": "If the high CPU usage is caused by the database process, it is usually due to poorly optimized SQL statements. This section focuses on CPU abnormalities caused by user statements.",
        "steps": "Step 1: If the CPU usage is consistently high, query the dbe_perf.statement and dbe_perf.summarystatement views to identify the statements with high CPU time.\nStep 2: If the CPU usage is currently high, use the pg_stat_activity view to get the query_id of the running SQL statement. Then use the query_id to query the pg_thread_wait_status view to get the Iwtid of the running SQL. Finally, use the 'top -Hp gaussdb_process_id' command to check the CPU usage of the corresponding lwtid (PID).\nStep 3: If the CPU usage was high in the past, refer to the performance jitter section of this chapter to identify the target SQL.\nStep 4: Query the statement history table on each CN/DN node to find slow SQL statements with high CPU consumption. Compare the cpu_time and db_time of the statements to identify the ones with high CPU consumption.\nStep 5: If the SQL statements found in the previous steps have intermittent high CPU consumption, use the dynamic interface to capture detailed information about the subsequent execution of the queries.",
        "metrics": "['cputime', 'query_id', 'Iwtid', 'cpu_usage', 'cpu_time', 'db_time']"
    },
    {
        "cause_name": "high_io",
        "desc": "High IO can be caused by user statements that result in excessive physical reads. It can be identified by checking the 'nblocks_fetched' and 'nblockshit' fields in the dbe_perf.statement/dbeperfsummary statement tables. If the difference between these two fields is high, it indicates a high number of physical reads.",
        "steps": "1. Check the 'nblocks_fetched' and 'nblockshit' fields in the dbe_perf.statement/dbeperfsummary statement tables. If the difference between these two fields is high, it indicates a high number of physical reads.\n2. Query the pg_thread_waitstatus view and check the 'waitstatus' and 'wait_event' fields. If the query status is 'IO_EVENT' or 'DataFileRead', it indicates the presence of physical reads.\n3. Query the dbe_perf.local_activesession/qsasp view or table for records where the query wait event is 'IO_EVENT' or 'DataFileRead'. Refer to the performance jitter section for more details.\n4. Query the slow SQL records with a high difference in 'n_blocks_fetched' and 'n_block_shit' fields, or high 'dataio_time' records. If the slow SQL has the 'L2details' field enabled, check the corresponding events (such as 'DataFileRead') in the 'events' field. Note: This capability is only available in kernel version 503.\n5. Use the dynamic interface (refer to the high CPU section in this chapter) along with step 4 to identify abnormal SQL.",
        "metrics": "['nblocks_fetched', 'nblockshit']"
    },
    {
        "cause_name": "high_memory_usage",
        "desc": "If the database kernel has high memory usage, it can lead to various issues such as query errors and abnormal memory consumption on user sessions.",
        "steps": "1. Check the 'dbe_perf.memory_nodedetail' view to determine the memory usage points, including 'max_process_memory' (maximum memory used by the process), 'process_used_memory' (memory already used by the process), 'max_dynamic_memory' (maximum dynamically usable memory), 'dynamic_used_memory' (used dynamic memory), and 'dynamic_used_shrctx' (used shared dynamic memory). Focus on the difference between 'max_dynamic_memory' and 'dynamic_used_memory'. If the dynamic memory is insufficient, it can cause query errors. 'dynamic_used_memory' includes memory consumption on user sessions and memory consumption by kernel modules.\n2. If 'dynamic_used_shrctx' is relatively small, query the 'dbe_perf.session_memory_detail' view to get the memory consumption of different sessions. Usually, the number of user sessions and the memory usage per session can cause dynamic memory issues.\n3. If 'dynamic_used_shrctx' is relatively large, query the 'dbe_perf.shared_memory_detail' view to identify the context of abnormal memory consumption. In most cases, it is due to abnormal memory consumption on user sessions.",
        "metrics": "['max_process_memory', 'process_used_memory', 'max_dynamic_memory', 'dynamic_used_memory', 'dynamic_used_shrctx']"
    },
    {
        "cause_name": "abnormal_wait_events",
        "desc": "Abnormal wait events can cause overall slowness in the system. It is important to identify these wait events and analyze if they are causing performance issues. Common approaches to reduce abnormal wait events can be found in the 'Overall Performance Slow - Wait Event Analysis' chapter (Chapter 1.2).",
        "steps": "Step 1: Check the current performance by querying 'pg_thread_wait_status' to identify the events most sessions are waiting for.\nStep 2: Check the past performance by querying 'dbe_perf.local_active_session' for recent slowness. For slowness within the last two days, query the 'gsasp' table in the 'postgres' database.\nStep 3: Investigate slow SQL queries by querying the 'statement_history' table and using the 'pg_catalog.statement_detail_decode(details, plaintext='true')' function to decode the details field (requires kernel version 503 or above). Switch to the 'postgres' database for this step.\nStep 4: For consistently slow performance, investigate the 'dbe_perf.wait_events' table and sort by 'total_waittime' or 'avg_waittime' in descending order. Identify the top events by referring to the 'Overall Performance Slow - Wait Event Analysis' chapter (Chapter 1.2).",
        "metrics": "['pg_thread_wait_status', 'dbe_perf_local_active_session', 'gsasp_table', 'statement_history_table', 'dbe_perf_wait_events']"
    },
    {
        "cause_name": "high_io",
        "desc": "High IO can cause performance degradation. It is important to identify the tables or queries that are causing high IO and optimize them.",
        "steps": "Step 1: Check the IO statistics of user tables by querying 'pg_stat_user_tables' and 'pg_stat_user_indexes'.\nStep 2: Check the IO statistics of user tables and indexes by querying 'pg_statio_user_tables' and 'pg_statio_user_indexes'.\nStep 3: Identify the tables or indexes with high IO and analyze the queries that are causing the high IO.\nStep 4: Optimize the queries or consider adding indexes to improve IO performance.",
        "metrics": "['pg_stat_user_tables', 'pg_stat_user_indexes', 'pg_statio_user_tables', 'pg_statio_user_indexes']"
    },
    {
        "cause_name": "abnormal_wait_events",
        "desc": "Abnormal wait events, including concurrent updates, can cause overall slowness in the system. It is important to identify these wait events and analyze if they are causing performance issues. Common approaches to reduce abnormal wait events can be found in the 'Overall Performance Slow - Wait Event Analysis' chapter (Chapter 1.2).",
        "steps": "Step 1: Check the current performance by querying 'pg_thread_wait_status' to identify the events most sessions are waiting for.\nStep 2: Check the past performance by querying 'dbe_perf.local_active_session' for recent slowness. For slowness within the last two days, query the 'gsasp' table in the 'postgres' database.\nStep 3: Investigate slow SQL queries by querying the 'statement_history' table and using the 'pg_catalog.statement_detail_decode(details, plaintext='true')' function to decode the details field (requires kernel version 503 or above). Switch to the 'postgres' database for this step.\nStep 4: For consistently slow performance, investigate the 'dbe_perf.wait_events' table and sort by 'total_waittime' or 'avg_waittime' in descending order. Identify the top events by referring to the 'Overall Performance Slow - Wait Event Analysis' chapter (Chapter 1.2).",
        "metrics": "['pg_thread_wait_status', 'dbe_perf_local_active_session', 'gsasp_table', 'statement_history_table', 'dbe_perf_wait_events']"
    },
    {
        "cause_name": "concurrency_issues",
        "desc": "Concurrency issues can cause performance degradation, such as increased response time, decreased TPS, or thread pool saturation.",
        "steps": "To diagnose concurrency issues, refer to the chapter on analyzing concurrency updates in the overall performance slow analysis. Check if there is a correlation between concurrency updates and performance degradation. If so, investigate the root cause and consider adjusting the concurrency settings or optimizing the thread pool.",
        "metrics": "['response_time', 'tps', 'thread_pool']"
    },
    {
        "cause_name": "concurrency_issues",
        "desc": "Concurrency issues can cause performance degradation, such as increased response time, decreased TPS, or thread pool saturation.",
        "steps": "To diagnose concurrency issues, refer to the chapter on analyzing concurrency updates in the overall performance slow analysis. Check if there is a correlation between concurrency updates and performance degradation. If so, investigate the root cause and consider adjusting the concurrency settings or optimizing the thread pool.",
        "metrics": "['response_time', 'tps', 'thread_pool']"
    },
    {
        "cause_name": "concurrent_update_delete",
        "desc": "If the business error is caused by concurrent UPDATE/DELETE operations on a partitioned table with row movement enabled, it can be resolved by disabling row movement or handling the business logic to avoid such conflicts.",
        "steps": "1. Check if the business error is related to concurrent UPDATE/DELETE operations on a partitioned table.\n2. If the error message indicates that row movement is causing conflicts, disable row movement for the table.\n3. If the business logic requires updating or deleting data across partitions, handle the business logic to avoid conflicts or consider changing the partition key.",
        "metrics": "[]"
    },
    {
        "cause_name": "concurrency_issues",
        "desc": "Concurrency issues can cause increased latency, decreased TPS, or thread pool exhaustion. This is mainly caused by concurrent updates and resulting lock waits.",
        "steps": "To diagnose concurrency issues, refer to Chapter 1.3 for guidance.",
        "metrics": "['latency', 'tps', 'thread_pool']"
    },
    {
        "cause_name": "concurrent_updates",
        "desc": "This event indicates concurrent updates. If there is a large number of these events, it may indicate a high level of concurrent updates causing overall business blocking. Use pg_thread_wait_status/dbe_perf.local_active_session/gs_asp to find the specific session_id causing the blockage.",
        "steps": "Check if there is a large number of concurrent update events. Use pg_thread_wait_status/dbe_perf.local_active_session/gs_asp to find the specific session_id causing the blockage.",
        "metrics": "['LOCK_EVENT/tuple', 'LOCK_EVENT/transactionid']"
    }
]