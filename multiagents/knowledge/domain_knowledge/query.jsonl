[
    {
        "cause_name": "operator_penalty_parameters",
        "desc": "The operator penalty parameters control the cost of certain operators in the query plan. By disabling these parameters, the optimizer will try to avoid selecting these operators in the execution plan.",
        "steps": "For each parameter, if it is set to 'OFF' or 'False', it means the corresponding operator is penalized. Disabling these parameters does not completely prevent the operators from being used in the plan, but the optimizer will try to avoid selecting them. If there are performance issues related to these operators, you can try enabling or disabling these parameters to see if it improves the query plan.",
        "metrics": "['enable_bitmapscan', 'enable_hashagg', 'enable_hashjoin', 'enable_index_nestloop', 'enable_material', 'enable_mergejoin', 'enable_nestloop', 'enable_seqscan', 'enable_sort']"
    },
    {
        "cause_name": "operator_cost_parameters",
        "desc": "The operator cost parameters control the relative size of the operator cost calculation, which can accurately control the operator selection potential.",
        "steps": "To increase the probability of selecting a plan, consider reducing the seq_page_cost and increasing the effective_cache_size parameters. Although the server allows setting the random_page_cost smaller than the seq_page_cost, it has no physical impact. If all databases are located in random access memory, it is reasonable to set them equal. In this case, non-sequential fetching pages have no side effects. Similarly, on databases with high cache hit rate, both values should be reduced relative to the seq_page_cost, as fetching pages from memory is much cheaper than usual. When setting the effective_cache_size parameter, consider the shared buffer of CnsfB and the kernel's strong cache, as well as the expected number of concurrent scans between different tables, as they will share the available space. This parameter does not affect the actual shared memory size allocated during the execution of Cusa08, it is only used for estimation in the sub-plan generation phase. This value is calculated using disk pages, usually with 8192 bytes per page.",
        "metrics": "['seq_page_cost', 'random_page_cost', 'effective_cache_size']"
    },
    {
        "cause_name": "operator_penalty_parameters",
        "desc": "The operator penalty parameters control the cost of certain operators in the query plan. By disabling these parameters, the optimizer will try to avoid selecting these operators in the execution plan.",
        "steps": "For each parameter, if it is set to 'OFF' or 'False', it means the corresponding operator is penalized. Disabling these parameters does not completely prevent the operators from being used in the plan, but the optimizer will try to avoid selecting them. If there are performance issues related to these operators, you can try enabling or disabling these parameters to see if it improves the query plan.",
        "metrics": "['enable_bitmapscan', 'enable_hashagg', 'enable_hashjoin', 'enable_index_nestloop', 'enable_material', 'enable_mergejoin', 'enable_nestloop', 'enable_seqscan', 'enable_sort']"
    },
    {
        "cause_name": "operator_cost_parameters",
        "desc": "The operator cost parameters control the relative size of the operator cost calculation, which can accurately control the operator selection potential.",
        "steps": "To increase the probability of selecting a plan, consider reducing the seq_page_cost and increasing the effective_cache_size parameters. Although the server allows setting the random_page_cost smaller than the seq_page_cost, it has no physical impact. If all databases are located in random access memory, it is reasonable to set them equal. In this case, non-sequential fetching pages have no side effects. Similarly, on databases with high cache hit rate, both values should be reduced relative to the seq_page_cost, as fetching pages from memory is much cheaper than usual. When setting the effective_cache_size parameter, consider the shared buffer of CnsfB and the kernel's strong cache, as well as the expected number of concurrent scans between different tables, as they will share the available space. This parameter does not affect the actual shared memory size allocated during the execution of Cusa08, it is only used for estimation in the sub-plan generation phase. This value is calculated using disk pages, usually with 8192 bytes per page.",
        "metrics": "['seq_page_cost', 'random_page_cost', 'effective_cache_size']"
    },
    {
        "cause_name": "poor_join_performance",
        "desc": "This code diagnoses poor performance in join operations. There are four main situations that can cause poor join performance: 1) when the GUC parameter 'enable_hashjoin' is set to 'off', which can result in the optimizer choosing NestLoop or other join operators even when HashJoin would be more suitable; 2) when the optimizer incorrectly chooses the NestLoop operator, even when 'set_hashjoin' is on; 3) when the join operation involves a large amount of data, which can lead to high execution costs; and 4) when the cost of the join operator is expensive. \n\nIn general, NestLoop is suitable when the inner table has a suitable index or when the tuple of the outer table is small (less than 10000), while HashJoin is suitable for tables with large amounts of data (more than 10000), although index will reduce HashJoin performance to a certain extent. Note that HashJoin requires high memory consumption.\n\nThe code checks for abnormal NestLoop, HashJoin, and MergeJoin operators, and identifies inappropriate join nodes based on the number of rows and cost rate. It also provides suggestions for optimization, such as setting 'enable_hashjoin' to 'on', optimizing SQL structure to reduce JOIN cost, and using temporary tables to filter data. \n\nIf the code finds any poor join performance, it is considered a root cause of the problem. Otherwise, it is not a root cause.",
        "metrics": "- total_cost\n- cost_rate_threshold\n- nestloop_rows_threshold\n- large_join_threshold"
    },
    {
        "cause_name": "complex_execution_plan",
        "desc": "This is a function that checks for complex execution plans in SQL statements. The function identifies two cases that may cause complex execution plans: (1) a large number of join or group operations, and (2) a very complex execution plan based on its height. If the function identifies either of these cases, it sets the corresponding details and suggestions for the user. If the number of join operators exceeds the \"complex_operator_threshold\" or the plan height exceeds the \"plan_height_threshold\", the function considers it a root cause of the problem. Otherwise, it is not a root cause.",
        "metrics": "- complex_boolean_expression\n- plan_parse_info\n- plan_parse_info.height\n- join_operator\n- len(join_operator)"
    },
    {
        "cause_name": "poor_aggregation_performance",
        "desc": "This function diagnoses poor aggregation performance in SQL queries. It identifies four potential root causes: (1) when the GUC parameter 'enable_hashagg' is set to 'off', resulting in a higher tendency to use the GroupAgg operator; (2) when the query includes scenarios like 'count(distinct col)', which makes HashAgg unavailable; (3) when the cost of the GroupAgg operator is expensive; and (4) when the cost of the HashAgg operator is expensive. The code checks for these conditions and provides detailed information and suggestions for each potential root cause. If none of these conditions are met, poor aggregation performance is not a root cause.",
        "metrics": "- total_cost\n- cost_rate_threshold\n- enable_hashagg\n- GroupAggregate\n- HashAggregate\n- Group By Key\n- NDV"
    },
    {
        "cause_name": "abnormal_sql_structure",
        "desc": "This function checks for a specific issue in the SQL structure that can lead to poor performance. If the rewritten SQL information is present, it indicates that the SQL structure is abnormal and can be a root cause of performance issues. The function provides a detailed description of the issue and suggests a solution to address it. If the rewritten SQL information is not present, it is not a root cause of the performance issue.",
        "metrics": "\n- rewritten_sql_info"
    },
    {
        "cause_name": "poor_index_filtering",
        "desc": "The query may have poor index filtering, leading to slow performance.",
        "steps": "1. Connect to the database.\n2. Query the execution plan using EXPLAIN statement.\n3. Identify the part of the statement with high execution time.\n4. If the execution plan shows that the query is scanning a large number of rows and filtering them afterwards, it indicates poor index filtering.\n5. Consider adding relevant indexes to the tables involved in the query to improve performance.",
        "metrics": "['execution_time', 'rows_scanned', 'rows_filtered']"
    },
    {
        "cause_name": "slow_sql_execution",
        "desc": "The SQL execution is slow and does not meet customer expectations and business requirements",
        "steps": "1. Identify the currently running SQL and their corresponding session ID, PID, and node name. 2. Determine the main wait events causing the slow SQL execution. 3. If most of the wait events are on the wait node, check the specific wait events on that node. 4. If the top wait event is related to acquiring locks, find the blocking session and investigate further. 5. Analyze the top wait events and combine them with the execution plan to identify the performance bottleneck. 6. Use the statement history to analyze historical slow SQL and identify the main time-consuming stages. 7. If the execution time does not exceed the log_min_duration_statement threshold, consider enabling full SQL tracking or using dynamic interfaces to capture the slow SQL.",
        "metrics": "['response_time', 'execution_time']"
    },
    {
        "cause_name": "poor_query_plan",
        "desc": "The query plan is not optimal, leading to slow SQL execution",
        "steps": "1. Obtain the query plan using the EXPLAIN command to quickly identify the performance bottleneck. 2. If the data_io_time is high, check if there are excessive physical reads and investigate if there are many dead tuples in the accessed table. 3. If the lock_wait_count is high, investigate possible lock conflicts and consider adjusting the lock timeout. 4. Determine the wait events consuming the most time for the slow SQL execution.",
        "metrics": "['data_io_time', 'lock_wait_count']"
    },
    {
        "cause_name": "poor_query_plan",
        "desc": "The query plan is not optimal, leading to slow SQL execution",
        "steps": "1. Obtain the query plan using the EXPLAIN command to quickly identify the performance bottleneck. 2. If the data_io_time is high, check if there are excessive physical reads and investigate if there are many dead tuples in the accessed table. 3. If the lock_wait_count is high, investigate possible lock conflicts and consider adjusting the lock timeout. 4. Determine the wait events consuming the most time for the slow SQL execution.",
        "metrics": "['data_io_time', 'lock_wait_count']"
    },
    {
        "cause_name": "suboptimal_sql",
        "desc": "Suboptimal SQL queries can negatively impact overall performance, potentially leading to thread pool saturation and other severe performance issues.",
        "steps": "To identify suboptimal SQL queries, use tools like pg_stat_activity to query for slow SQL queries and analyze their execution time. Additionally, monitor the number of slow SQL queries over time using OPS metrics. If there is a significant increase in the number of slow SQL queries, investigate their impact on overall response time using P90/95/99, TS, and C5 metrics. Refer to the chapter on analyzing slow SQL queries for more details.",
        "metrics": "['slow_sql', 'thread_pool']"
    },
    {
        "cause_name": "slow_sql_execution",
        "desc": "The SQL execution is slow and does not meet customer expectations and business requirements",
        "steps": "1. Identify the currently running SQL and their corresponding session ID, PID, and node name. 2. Determine the main wait events causing the slow SQL execution. 3. If most of the wait events are on the wait node, check the specific wait events on that node. 4. If the top wait event is related to acquiring locks, find the blocking session and investigate further. 5. Analyze the top wait events and combine them with the execution plan to identify the performance bottleneck. 6. Use the statement history to analyze historical slow SQL and identify the main time-consuming stages. 7. If the execution time does not exceed the log_min_duration_statement threshold, consider enabling full SQL tracking or using dynamic interfaces to capture the slow SQL.",
        "metrics": "['response_time', 'execution_time']"
    },
    {
        "cause_name": "suboptimal_query_plan",
        "desc": "The query plan for the slow SQL is suboptimal, leading to poor performance",
        "steps": "1. Obtain the execution plan for the slow SQL. 2. Analyze the execution plan to identify any suboptimal steps or operations. 3. Optimize the query plan by adjusting query parameters or rewriting the SQL statement.",
        "metrics": "['execution_plan']"
    }
]